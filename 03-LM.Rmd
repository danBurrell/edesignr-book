# Statistical Models

One of the great scientific achievements humanity has made is the ability to conceptualize complex phenomena and processes in the natural world through models that amplify the pertinent facts and relationships of interest and provide a logical platform from which to study systems that drive decision-making. Most scientific models can be expressed as mathematical abstractions. Two broad classes of mathematical models are **deterministic models** and **stochastic models**. Broadly speaking, deterministic models leave nothing to chance and blatantly ignore uncertainties. For example the concentration $c$ of a pollutant in a river at a point $x$ and time $t$ can be modeled as:
$$
c(x,t)=c_0 (x-vt)e^{-kt},
$$
where $c_0(x)$ is the initial pollutant concentration at point $x$, $v$ is the water velocity, and $k$ is a proportionality constant, measuring the efficiency of bacterial decomposition of the pollutant. This is a deterministic model: given inputs $x, v$ and $c_0$, the pollutant concentration at location $x$ and time $t$ is predicted with certainty. Such a model makes a very strong assumption that the model is correct and the inputs are measured with perfect precision and accuracy. But the pollutant concentration makes certain assumptions: (i) that the pollutant concentration is uniform in all directions except downstream flow, (ii) there are no diffusive effects due to to contour irregularities and turbulence, (iii) the pollutant decays as a negative exponential due to bacterial action, (iv) the bacterial efficiency is time-homogeneous, and (v) there is thorough mixing of the pollutant in the water. These assumptions are reasonable, but they not necessarily (always) true. The uncertainty of the effects at a particular location along the river and point in time can be incorporated by casting the model stochastically:
$$
c(x,t) = c_0 (x-vt)e^{-kt} + e
$$
where $e \sim D(0,\sigma^2)$, for some probability distribution $D$. Allowing for the random deviation $e$ we are now claiming that $c(x,t)$ is a random variable with expectation:
$$
\mathbb E[c(x,t)] = c_0 (x-vt)e^{-kt} 
$$

## Linear Models in R

Most R functions uses a formula representation to represent regression models (for continuous variables) or means and effects models (for categorical variables):

```{r, eval=FALSE}
response ~ model
```

where the tilde (~) defines a model formula and `model` represents a set of terms to include as predictors in the model. Terms are included by their variable names and various operators such as `+` (include in model), `-` (exclude from model), `:` (interaction), `*` (full factorial structure), and so on. The intercept term (which can be explicitly denoted in a model as a `1`) is implicitly defined and need not be specified (although I often do for completeness). Thus the following model formulae are equivalent (all include the intercept):

```{r, eval=FALSE}
Y ~ X
Y ~ 1 + X
Y ~ X + 1
```

whereas these formulae express different ways of excluding the intercept:

```{r, eval=FALSE}
Y ~ -1 + X
Y ~ X - 1
```

Linear models are fitted using the method of ordinary least squares by providing the model formula as an argument to the `lm()` function. For example, we could generate a fictitious response variable (`Y`) and a fictitious continuous predictor (`X`) as follows:

```{r}
# Load packages
if(!require(pacman)) install.packages("pacman")
library(pacman)
p_load(tidyverse)

# Generate data set
N = 10
fictitious_data = tibble(
   X = 1:N,
   Y = 0.9*X + rnorm(n=N, mean=0, sd=0.5)
)

# Plot data
ggplot(data = fictitious_data,
       mapping = aes(x=X, y=Y)) +
   geom_point(colour = "blue") +
   geom_smooth(method = "lm",
               formula = y ~ x,
               se = FALSE,
               colour = "darkgreen") +
   theme_classic()
```
The plot shows the data points and a simple linear regression line fit through them. To fit the regression in such a way as to be able to work with the estimated parameters we use `lm()`:

```{r}
# Fit simple linear regression model
fictitious_lm = lm(
   formula = Y ~ 1 + X,
   data = fictitious_data)
```

To examine the estimated parameters (and hypothesis tests) from the fitted model we can use the function `summary()`:

```{r}
# Summary of regression 
summary(fictitious_lm)
```

There is a lot of useful information presented here, however it is not necessarily easy to access and work with in successive computations. This is where the `broom` package comes in. There are three main functions that allow us to extract pertinent information and store it in data frames, for easy access later. The functions are:

* `tidy()`: returns the statistical findings of the model (such as coefficients)
* `glance()`: returns a concise one-row summary of the model
* ``augment()`: adds prediction columns to the data being modeled

Using `tidy(fictitious_lm)` we can extract the pertitent statistical information such as estimates, standard errors, test statistics and p-values:

```{r}
# Ensure broom package is loaded
p_load(broom)

# Set output digits to 3 for printing
options(digits=3)

# tidy() output
tidy(x = fictitious_lm,
     conf.int = TRUE,
     conf.level = 0.95)
```
From this we see, for example, that the simple linear regression equation is given by:
$$
\hat Y_i = 0.387 + 0.871\times X_i,  
$$
and that we cannot reject $H_0: \beta_0=0$ (p = 0.334), but we strongly reject $H_0:\beta_1 = 0$ (p = 0.000000536), indicating that a model with no zero intercept will be just as good:

```{r}
fictitious_lm.2 = lm(
   formula = Y ~ -1 + X,
   data = fictitious_data)

# Compare the full model to the reduced model with zero intercept
anova(fictitious_lm, fictitious_lm.2) %>% tidy()
```
We see that there is no significant reduction in sums of squares between the model with the unconstrained intercept and the model with the intercept constrained to be zero. We therefore opt to use the latter model:

```{r}
tidy(x = fictitious_lm.2,
     conf.int = TRUE,
     conf.level = 0.95)
```
which has regression equation:
$$
\hat Y_i=0.927\times X_i
$$

Using `glance(fictitious_lm.2)` we can obtain a one-row summary of the reduced model. For the linear model this summary contains various statistics about the fit of the model, such as the residual standard error and $R^2$:

```{r}
# One-row summary at a glance
glance(x = fictitious_lm.2)
```

Using `augment(fictitious_lm.2)` we get back an observation-level data frame containing the original data used to fit the model as well as fitted values (`.fitted`) and standard errors (`.se.fit`). 

```{r}
augment(x = fictitious_lm.2,
        newdata = NULL,
        se_fit = TRUE,
        interval = "none") # confidence, prediction
```
Notice that `augment()` also appends model-specific statistics that enable deeper exploration of the fit of the model. For example, we can explore how well the model fits by plotting the observed values `Y` against the fitted values `.fitted` (compare this to the plot constructed earlier, which was based on the full model with unconstrained intercept).

```{r}
p1 = augment(x = fictitious_lm.2) %>%
   ggplot(mapping = aes(x = X)) +
   geom_point(mapping = aes(y = Y), colour="blue") +
   geom_line(mapping = aes(y = .fitted), colour="magenta") +
   theme_classic()

p1
```
We could overlay the unconstrained model as follows:
```{r}
ggplot(data = fictitious_data,
       mapping = aes(x=X, y=Y)) +
   geom_point(colour = "blue") +
   geom_smooth(method = "lm",
               formula = y ~ x,
               se = FALSE,
               colour = "darkgreen") +
   geom_smooth(method = "lm",
               formula = y ~ -1 + x,
               se = FALSE,
               colour = "magenta") +
   theme_classic()
```

## Estimating linear model parameters

During model fitting, parameters can be estimated using a variety of estimation methods. The methods of ordinary least squares (OLS), maximum likelihood (ML) and restricted maximum likelihood (REML) are the most common. The OLS estimates of parameters minimize the sum of squared deviations between the observed and fitted values (a function of the model parameters). There are several variants on the method of ordinary least squares: weighted least squares can accommodate data that varies in quality and generalized least squares can accommodate heterogeneous variances and correlated data. The model we've been looking at is usually referred to as the general linear model (and it was once the most general linear model, but it is not very general by today's standards). Broadly speaking we consider such a model with purely continuous predictor variables to be a **regression** model, while a model with purely categorical predictors is an **analysis of variance (ANOVA)** model. Models that incorporate both continuous and categorical predictors are **analysis of covariance (ANCOVA)** models.  

ML estimators estimate model parameters such that the (log) likelihood of obtaining the observed data is maximized. Under standard assumptions (normal data, constant variance, independent observations) the ML estimates are identical to the OLS estimates. However, maximum likelihood parameter estimation extends beyond the general linear model to the so-called "generalized" linear model, which are not restricted to normally distributed errors and response. Instead they generalize to any distribution belonging to the family of distribution models called exponential dispersion models (which include normal, binomial, Poisson, gamma, negative binomial and others).  

